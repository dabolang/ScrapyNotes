# 一个简单的Sprapy实例

## 前言

初学Scrapy本想看看官方教程,无奈我和官网之间隔了一道墙,懒得翻过去，直接去GitHub找到[1.5版文档](https://github.com/scrapy/scrapy/tree/1.5/docs)还有[参考项目](https://github.com/scrapy/quotesbot),先把demo成功的运行起来,再去理解Scrapy中的概念。
接下来按照教程自己动手做了一遍。

## 开始我的第一个Scrapy项目

我们将要爬取 quotes.toscrape.com，该网站列出了一些名人名言。

首先看下即将完成的任务:

- [创建一个新的Scrapy项目](#创建一个项目)
- [编写一个spider类进行网站爬取和提取数据](#创建spider)  

## 创建一个项目

在爬取数据之前，你应该先建立一个新的Scrapy项目.输入一个你喜欢的目录名称，接下来你的代码将放在这个目录下面:

```cmd
scrapy startproject tutorial
```

执行结果

```cmd
PS D:\ScrapyNotes\note_1> scrapy startproject tutorial
New Scrapy project 'tutorial', using template directory 'c:\\users\\xxx\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\scrapy\\templates\\project', created in:
    D:\ScrapyNotes\note_1\tutorial

You can start your first spider with:
    cd tutorial
    scrapy genspider example example.com
```

执行这个命令,你将会得到一个名为 ``tutorial`` 的目录，具体结构如下:

```pwd
     tutorial/
        scrapy.cfg            # 项目的配置文件
        tutorial/             # 该项目的python模块。之后您将在此加入代码。
            __init__.py
            items.py          # 项目中的item文件。
            middlewares.py    # 项目中的中middleware文件。
            pipelines.py      # 项目中的pipelines文件。
            settings.py       # 项目的设置文件
            spiders/          # 放置spider代码的目录。
                __init__.py
```

> 注意以后的scrapy命令都需要在此目录`tutorial`下进行输入

## 创建Spider

接下来，我们会定义一个Spider类，Scrapy 将会使用这个类去爬取网站上的信息.这个类必须继承自 scrapy.Spider 同时， 你还需要定义一些其他的方法，比如说，创建初始化的请求，你也可以选择如何去跟进这个网页的其他链接，以及如何从已下载的网页中解析提取相应的数据。

这是我们的第一个Spider代码.将这些代码保存至 tutorial/spiders 下的 quotes_spider.py 文件中:

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split("/")[-2]
        filename = 'quotes-%s.html' % page
        with open(filename, 'wb') as f:
            f.write(response.body)
        self.log('Saved file %s' % filename)
```

如你所见，我们的Spider继承了 scrapy.Spider 同时还定义了一些属性和方法:

- name: 该属性是用来唯一标识一个Spider的，所以你不能在不同的Spider中使用相同的name.
- start_requests(): 该方法必须返回一个可迭代的Requests（你可以返回一个列表或者编写一个生成器函数），保证一连串的请求将会被这个方法成功的创建，随后，Spider 将会从这里开始他的爬虫之旅.
- parse():

我们在项目文件夹下打开命令行，输入`scrapy list`命令查看所有的Spider,实例中会显示`quotes`

```cmd
> scrapy list
quotes
```

然后输入下面命令运行name为`quotes`的Spider

```cmd
scrapy crawl quotes
```

> 成功则在项目文件夹下生成`quotes-1.html和quotes-2.html`两个文件，怎么样够简单吧。
